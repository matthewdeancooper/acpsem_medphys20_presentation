* Presentation
** Introduction (45s)
  - Hello everyone my name is Matthew Cooper. I a am recent
    medical physics graduate from the university of Sydney.

  - This project was completed in collaboration with Simon Biggs and Matthew
    Sobolewski from the RCCC, as well as with Yu Sun from the university of Sydney.

  - The title of todays presentation is "A clinical implementation of deep
    learning: Automatic contouring via U-Net architecture".

  - In this presentation I plan to spend about 10 minutes walking through our
    research from clinical need to deployment.

  # - During my masters project we developed multiple contouring models
  #   and were able to
  # - A link to the supporting thesis is included on this slide as well as a video
  #   overview of the implementation hosted on the PyMedPhys documentation page.

** Limitations (1.3m)
   - Accurate contouring is a critical aspect of safe and effective treatment
     delivery in radiotherapy. However, there are some current limitations.

   - There exists variance between medical experts in the definition of
     anatomical regions from medical images. This is refereed to as inter
     observer variance or IOV and is often cited as the largest source of error in
     treatment delivery.
   # - In 2020 an AAPM task group risk assessment highlighted
   #   multiple human-factor failure modes in RT relating to contour generation.

   - In addition, current time contrasts from traditional atlas techniques
     are a barrier to the adoption of new technologies such as adaptive radiotherapy that
     require fast contouring

   - Recently, deep learning methods have demonstrated the potential to overcome these
     limitations with significant improvements over atlas based methods
     in both time and accuracy and are now considered the SOTA in research.

** Application ()
   - This study designed and evaluated a 2D U-Net architecture with two primary
     aims:

    1) Develop a pelvic imaging quality QA tool, comparing model
       predictions with expert contours to identify macro contouring errors.
       Here we contoured patient, bladder and rectum regions.

    2) To automate a time consuming aspect of canine RT. Vacuum bags were
       delineated manually as the small animal hospital with a reported
       completion time of 30 minutes per patient.

       We focused model deployment on an animal hospital, as this was the
       fastest way to get a product into the hands of clinicians for feedback.

    - To achieve these aims would require performance similar to human experts.
      To evaluate our model we used the surface dice similarity coefficient. A
      relatively new metric that takes into account expert variance specific to
      each organ and returns the fraction of surface points that are within
      IOV tolerances.

    - We made use of this metric as the literature indicated a stronger
      correlation with correction time when compared to traditional metrics
      such as the volumetric DSC

** sDSC vs DSC ()

** Model architecture ()
   For those that havent seen before, this is what a 2D U-Net architecture looks
   like - I only want you to get a high level overview here.

   It is composed of two primary pathways:

   1 - On the LHS we have the encoding pathway (in blue) that down-samples the resolution
   of the input at each level while increasing the number of features that have
   been extracted via convolutional operations.
   2 - On the RHS we have the decoding pathway (in yellow) that up-samples
   low resolution features, concatenates them with higher resolution features sent
   from the residual connection.

   If you remember one thing from this busy slide I want you to notice the
   pattern of decreasing the spatial resolution, recovering the spatial
   resolution, and concatenating it with higher resolution features.


# ** Why down sample
#    1 - Reduces the total size of feature representations. Currently there are
#    hard GPU memory constraints that limit the depth, resolution, and complexity
#    of model architecture. The success in computer vision is in part due to
#    convolutional operations encoding some fundamental assumptions about our data
#    into our model and reducing the number of trainable parameters when compared
#    to fully connected networks.

#    2 - Additionally, leverage down-sampling to facilitate multi-resolution
#    analysis. If you imagine keeping the size of a convolutional kernel
#    constant - seen in grey - while reducing the resolution of the image, we are
#    effectively increasing the relative size of the kernel, allowing for the
#    extraction of spatially broader features (general localisation) without the
#    memory overhead that a larger kernel would include. By concatenating together
#    multi-resolution feature representations we are able to detect, localise, and
#    produce high-resolution border segmentation.

#    7 minutes


** Modules required ()

   Deep learning models share a common set of modules that can be broken down into
   data wrangling, training, and deployment.
   This slide lists the steps or coding packages that we had to develop in order
   to translate from an idea to part of the clinical workflow.

   Now for the most part there is enough information online about how to deal
   with data wrangling and training. However, I found less information about how
   to successfully deploy a model to a clinical environment - so I would like to
   focus on deployment, but I am happy to answer any questions everyone has.

** Deployment ()

   We were unable to preform inference locally at clinic A due to hardware
   constrains. Therefore, we bridged a connection to another clinic via SSH and
   were then able to communicate between sites by sending network requests to a
   local IP address using the same port.

   We used pynetdicom as our networking protocol for building DICOM service
   class users and providers. A provider includes instructions for completing a
   task - i.e., how to store a set of images.
   While a class user requests to utilise the functionality of a provider -
   i.e., can you store these images.

   The treatment planning system exports an imaging series to the dicom server
   by issuing a storage request. After storage, the server forwards the series to a
   tensorflow model that infers the contours. These contours are translated to a
   dicom rt structure file that is then forwarded back to the TPS by another
   storage request.

** Results ()

** Structure specific metrics ()

** Conclusion ()
