* Presentation
** Introduction (15s)

  - Hello everyone my name is Matthew Cooper and I am recent graduate from the
    master of medical physics degree at the university of Sydney

  - The title of today's presentation is "A clinical implementation of deep
    learning: Automatic contouring via U-Net architecture".

** Limitations (51s)

   - Accurate contouring is a critical aspect of safe and effective treatment
     delivery in radiotherapy. However, there are current clinical limitations.

   - There exists variance in the definitions of anatomical regions between
     medical experts from imaging data. This is referred to as inter
     observer variance or IOV and is often cited as the largest source of error
     in treatment delivery.

   - In addition, a TG275 risk assessment highlighted multiple
     human-factor failure modes in RT relating to contour generation.

   - There are also time contrasts involved in contouring and traditional computer
     aided techniques such as atlas methods still require significant correction
     times.

   - Recently, deep learning models have demonstrated the potential to overcome
     these limitations. When used as a starting reference they have been shown
     to reduce variance between observers and decrease total contouring time.
     They have also demonstrated significant improvements compared to atlas
     methods in both time and accuracy.

** Application (50s)

   - In an attempt to meet these limitations, this study designed and evaluated a
     2D U-Net architecture with two primary aims:

   - Model 1 was designed as a QA tool for prostate cancer, we compared expert
     contours with model predictions in an attempt identify macro contouring
     errors. Here we contoured patient, bladder and rectum volumes.

   - Model 2 was designed to automate a time consuming aspect of canine RT,
     vacuum bag segmentation. Previously, vacuum bags were delineated manually
     at clinic with a reported completion time of 30 minutes per patient.

   - To achieve these aims would require model performance similar to that of
     human experts. To quantify performance we used the surface dice
     coefficient, which is a metric that takes into account the variance between
     experts in its assessment. And fundamentally we are interested in this
     metric as the literature has indicated a stronger correlation with
     corrections times when compared to the traditional dice coefficient.

** sDSC vs DSC (50)

   - Now, If we compare the SURFACE dice with the traditional dice coefficient
     we see that the dice takes the intersections of two masks and returns the
     fraction of volumetric overlap.

   - In comparison the surface dice defines a boundary, adds an organ specific
     tolerance representative of IOV and return the fraction of surface points
     that are withing this tolerance and theoretically the fraction of surface
     points that would not need to be corrected manually.

** Modules required (54s)

  - So, Deep learning models share a common set of modules that can be broken
    down into data wrangling, training, and deployment stages.

  - In terms of our dataset, each model we used an average of 20 patients split
    into training, validation and testing subsets.

  - In our training stage, we made heavy use of data augmentation to increase
    the spread of our data distribution

  - We then designed a 2D U-Net architecture by incorporating recent
    modifications in the literature that indicated performance improvements.

  - Additionally, we assessed the performance of multiple cost functions and
    selected weighted dice cost as it was the only loss function that optimised
    for all pelvic organs due to a significant pixel-wise class imbalance
    between structures

** Deployment (48s)

  - In terms of model deployment, we were unable to preform inference locally at
    clinic A due to hardware constraints. Therefore, we bridged a connection to
    another clinic via an encrypted tunnel and were then able to communicate
    between sites via the DICOM networking protocol.

  - In practise, the treatment planning system exports an imaging series to a
    remote DICOM server by issuing a storage request. After storage, the server
    forwards the series to a tensorflow model that infers the contours. These
    contours are translated to a dicom rt structure file that is then forwarded
    back to the TPS by another storage request.

  - This software was designed to handle multiple requests by storings jobs in
    an inference queue.

** Results (1.30m)

  - Provide a visual overview of the contours generated by each model and how
    they compare to experts. This is the pelvic imaging model. On the input
    image the truth is outlined in yellow and the model prediction is outlined
    in red. There is also a difference column that graphs a subtraction.

*** Pelvic imaging model

  - For patient contours we saw excellent agreement with the worst case observed
    having a volumetric overlap score of over 99 percent.

  - Bladder contours we observed some room for improvement. A recurring theme
    of predictions was the under representation the posterior aspect of the
    bladder.

  - The worst case recorded a volumetric overlap of 67 percent, with the
    surface coefficient indicating 30 percent of the border needed manual
    correction. We suspect a broader dataset may improve performance here.

  - As for rectum contours, again, we observed some room for improvement.

  - Specifically, rectum regions containing gas were not correctly identified by
    the model. We suspect an architecture that accepts 3D input may provide the
    axial context required to interpolate the existence of the rectum in these
    cases.

*** Canine imaging model

  - We are now examining vacuum bag contours from the canine model and again we
    observed excellent agreement between model and expert with the lowest
    scoring contours achieving a volumetric overlap of 90 percent.

** Structure specific metrics (1m)

 - Quantifying average values for each organ we observed that patient and
   vacuum bag segmentation were within tolerances. Specifically the vacuum bag
   contours have been accepted clinically under the condition that they are
   verified by an RT.

 - The literature defines clinically acceptable bladder and rectum agreement to
   be a DSC greater than 0.7 - a 70 percent volume overlap. On average, were able achieve
   this with contours. However in practise experts are able to achieve stronger
   better agreement than this.

 - However, in each case the sDSC indicates only 10 percent of border points
   would need to be adjusted to be within the top 95th percentile of expert
   agreement. This should correlate with low corrections times for these
   contours.

 - The take home message from this slide is that while the rectum and bladder
   contours would aid clinicians as a starting reference, higher performance is
   required before they have utility in a QA tool.

** Conclusion and future research (1.40m)

 - Patient contours within tolerances and are viable for use within the QA tool.

 - Bladder and rectum contours may improve with a broader dataset. SOTA
   implementations use at least an order of magnitude more data.

 - In addition, 3D implementations may improve detection

 - The canine imaging model was successfully deployed to clinic under a
    prototype warning that requests manual verification. Acceptance testing has
    shown a performance improvement of 30 minutes per patient. Currently being
    utilised on all new canine patients.

 - sDSC indicated stronger correlation with correction times in lit. To minimise
   correction times we want to optimise for this directly. Currently accepts
   only binary data so a continuous surrogate is needed for gradients to
   be defined during training.

** Other
# ** Model architecture ()
#    For those that havent seen before, this is what a 2D U-Net architecture looks
#    like - I only want you to get a high level overview here.

#    It is composed of two primary pathways:

#    1 - On the LHS we have the encoding pathway (in blue) that down-samples the resolution
#    of the input at each level while increasing the number of features that have
#    been extracted via convolutional operations.
#    2 - On the RHS we have the decoding pathway (in yellow) that up-samples
#    low resolution features, concatenates them with higher resolution features sent
#    from the residual connection.

#    If you remember one thing from this busy slide I want you to notice the
#    pattern of decreasing the spatial resolution, recovering the spatial
#    resolution, and concatenating it with higher resolution features.


# ** Why down sample
#    1 - Reduces the total size of feature representations. Currently there are
#    hard GPU memory constraints that limit the depth, resolution, and complexity
#    of model architecture. The success in computer vision is in part due to
#    convolutional operations encoding some fundamental assumptions about our data
#    into our model and reducing the number of trainable parameters when compared
#    to fully connected networks.

#    2 - Additionally, leverage down-sampling to facilitate multi-resolution
#    analysis. If you imagine keeping the size of a convolutional kernel
#    constant - seen in grey - while reducing the resolution of the image, we are
#    effectively increasing the relative size of the kernel, allowing for the
#    extraction of spatially broader features (general localisation) without the
#    memory overhead that a larger kernel would include. By concatenating together
#    multi-resolution feature representations we are able to detect, localise, and
#    produce high-resolution border segmentation.
